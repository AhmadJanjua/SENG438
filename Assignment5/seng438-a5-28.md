**SENG 438- Software Testing, Reliability, and Quality**

**Lab. Report \#5 – Software Reliability Assessment**

| Group: Group Number   28  |
|-----------------|
| Student 1 name:   Ahmad Janjua        |   
| Student 2 name:   Maxwell Kepler      |   
| Student 3 name:   Christopher Luk     |   
| Student 4 name:   Matthew Ho          | 

[1 Introduction	](#introduction)

[2 Assessment Using Reliability Growth Testing ](#assessment-using-reliability-growth-testing)

[3 Assessment Using Reliability Demonstration Chart ](#assessment-using-reliability-demonstration-chart)

[4 Comparison of Results ](#comparison-of-results)

[5 Discussion on Similarity and Differences of the Two Techniques ](#discussion-on-similarity-and-differences-of-the-two-techniques)

[6 How the team work/effort was divided and managed ](#how-the-team-workeffort-was-divided-and-managed)

[7 Difficulties encountered, challenges overcome, and lessons learned ](#difficulties-encountered-challenges-overcome-and-lessons-learned)

[8 Comments/feedback on the lab itself ](#commentsfeedback-on-the-lab-itself)

# Introduction
Within this lab, the main objective was to work with a hypothetical system and its provided failure data, gaining practical experience in using both C-SFRAT and RDC-11 to analyze, interpret, and compare results with the goal of teaching new skills and knowledge needed to effectively assess software reliability and address potential issues in real-world scenarios. This lab is divided into two main sections. Within the first section, students will explore reliability growth testing by installing a suitable assessment tool and creating plots of the failure rate and reliability of the SUT. Within the second portion of the lab, the students will delve into the usage of the Reliability Demonstration Chart (RDC), which offers an effective method for assessing whether the target failure rate of MTTF is met.

# Assessment Using Reliability Growth Testing 

The Regression Growth Analysis is shown in three different charts. The first three charts are the data we gathered using C-SFRAT. The first graph shows the MVF with multiple models displayed. The first diagram we modeled all the hazard functions and saw how they matched up to our input data. The second graph used shows the two best models of MVF where we evaluated truncated logistic IFR generalized salvia and bollinger model. These are hazard functions which are also known as failure rate functions and are functions that describe the rate at which events occur over time. The final graph shows the intensity of failures within the system. In this model we can see a rise in intensity of failures within the data and then an eventual drop to its lowest point.

The next three graphs represent the data applied to some of the formulas discussed in class and then plotted into excel. The first graph represents the time between failures within the system. We can see that the time between failures is much higher near the end. The second graph shows the failure intensity. The amount of failures in the system sharply rises up, then drops slightly, and then plateaus. The final graph represents the reliability which we can see a high amount of reliability at the beginning and then the reliability exponentially decreases near the end.

# Assessment Using Reliability Demonstration Chart 

# Comparison of Results
While both the RGA and RDC methodologies shared some similarities, there were key differences that led to significant changes in their processes. When comparing RDC and RGA, RDC is a subset of RGA because it focuses on a more specific aspect of reliability assessment. When looking deeper into RGA, RGA offers a broader and more in-depth analysis of system reliability using additional data sources. Additionally, RGA placed a strong emphasis on analyzing failure intensity, which enables us to concentrate on failures in relation to time. On the other hand, RDC offered more insights into the mean time to failure (MTTF), which allowed us to identify the minimum MTTF. By getting the MTTF, we can understand if the SUT was capable of operating effectively. 

When looking at our results, within the RDC we have a clear objective on what we’re looking for but it is set to only one objective. With the RGA the data is very broad but we are able to extrapolate from the data to be able to determine minute details. 


# Discussion on Similarity and Differences of the Two Techniques

To understand what the similarities and differences are between the two techniques it may be strategic to first define what each of the respective graphs represents. The Reliability Growth Analysis is a test that is based on internal failure time and/or failure count and target failure rate. The Reliability Demonstration Chart is more of a subset representation of the Reliability Growth Analysis in the fact that it is based on internal failure times and target failure rate. It is therefore obvious that both charts are similar in the fact that both charts show the failure time and the target failure rate. They are different in the fact that the Reliability Growth Analysis has the potential to also show the failure counts.In retrospective, the Reliability Growth Analysis has more information and focuses on the specifics within the data while in comparison, the Reliability Demonstration Chart is a much simpler model but allows for an easier interpretation of the data. The Reliability Growth Chart has more freedom with the data as opposed to the Reliability Demonstration Chart which shows a subset of the original chart and is much more specific but is also easier to interpret.

# How the team work/effort was divided and managed

To streamline our lab work, we organized it into two different groups: part one and part two. Each section was assigned to two team members to handle. For the reliability growth testing, we split the tasks between two people. One team member used C-SFRAT to generate graphs, while the other team member analyzed the graphs and wrote detailed descriptions of the results. We followed a similar approach for the reliability demonstration chart. This strategy allowed us to efficiently complete our work while also ensuring consistency in our results.

# Difficulties encountered, challenges overcome, and lessons learned

The difficulties we faced were related to the provided software files. Specifically, SRTAT and the Reliability Demonstration Chart Excel sheet did not function as intended according to the lab handout. Further details on this issue can be found in the feedback section. Ultimately, for part one we had to resort to using C-SFRAT and for part two, we used the Reliability Demonstration Chart Excel sheet.

# Comments/feedback on the lab itself

The technical difficulties with the resources provided made this lab the most challenging one. Specifically, the SRTAT program failed to execute even after the CSV file provided in the lab artifacts was reformatted multiple times. We also tested other programs that were mentioned in the lab, but they either did not work on MAC or failed to accept the data set without heavy modification. With minimal instructions given in the lab document, navigating this issue was difficult and some team members were rendered useless. While the RDC document instructions seemed straightforward upon reading and opening the Excel sheet, the document did not function as expected. Most of the instructions were impossible to complete due to the protected view in which most of the document was displayed. Additionally, it was not possible to add data into the crucial cumulative failure count column without unprotecting the worksheet, adding to the frustration of the assignment. Overall, it’s deplorable that this assignment hasn’t been updated over the years. The challenge of the assignment shouldn’t come from a lack of concise instructions or an organized dataset, but rather from learning proper methods of analysis through clear instructions. 
