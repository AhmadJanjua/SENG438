**SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#4 – Mutation Testing and Web app testing**

| Group: Group Number   28  |
|-----------------|
| Student 1 name:   Ahmad Janjua        |   
| Student 2 name:   Maxwell Kepler      |   
| Student 3 name:   Christopher Luk     |   
| Student 4 name:   Matthew Ho          | 

[1 Introduction	](#introduction)

[2 Analysis of 10 Mutants of the Range class](#analysis-of-10-mutants-of-the-range-class)

[3 Report all the statistics and the mutation score for each test class](#report-all-the-statistics-and-the-mutation-score-for-each-test-class)

[4 Analysis drawn on the effectiveness of each of the test classes	](#analysis-drawn-on-the-effectiveness-of-each-of-the-test-classes)

[5 A discussion on the effect of equivalent mutants on mutation score accuracy](#a-discussion-on-the-effect-of-equivalent-mutants-on-mutation-score-accuracy))

[6 A discussion of what could have been done to improve the mutation score of the test suites	](#a-discussion-of-what-could-have-been-done-to-improve-the-mutation-score-of-the-test-suites)

[7 Why do we need mutation testing? Advantages and disadvantages of mutation testing](#why-do-we-need-mutation-testing-advantages-and-disadvantages-of-mutation-testing)


[8 Explain your SELENUIM test case design process](#explain-your-selenuim-test-case-design-process)

[9 Explain the use of assertions and checkpoints](#explain-the-use-of-assertions-and-checkpoints)

[10 how did you test each functionaity with different test data](#how-did-you-test-each-functionaity-with-different-test-data)

[11 Discuss advantages and disadvantages of Selenium vs. Sikulix.](#discuss-advantages-and-disadvantages-of-selenium-vs-sikulix)

[12 How the team work/effort was divided and managed](#how-the-team-workeffort-was-divided-and-managed)

[13 Difficulties encountered, challenges overcome, and lessons learned](#difficulties-encountered-challenges-overcome-and-lessons-learned)

[14 Comments/feedback on the lab itself](#commentsfeedback-on-the-lab-itself)



# Introduction

In this lab we will be focused on two types of testing: mutation and selenium testing. Mutation testing involves making small changes or mutations to the code to see if it still works as intended, while selenium testing involves using a framework to automate web testing. By utilizing both these testing methods, we aim to achieve a more thorough and efficient understanding of the testing process that can help detect errors and improve the quality of the software in question.

# Analysis of 10 Mutants of the Range class 
1. Mutation: Parameter 2 is NULL in combine method
This mutation survived because in the original test suite conducted in assignment 3, we tested combine but failed to test a situation where the range of the second parameter in the combine method would be NULL. In order to fix this we created a new test case that would set range 2 to null and place it into the second input parameter of combine. This mutation was then killed as we saw range 1 being returned.

2. Mutation: Value is in between range in method contains
This mutation survived because in the original test suite conducted in assignment  3, we did not test a situation where the value is inside of the range. Therefore there was simply no mutation coverage for this method. To kill this mutation we simply created a range between (-6,5) and then inputted a value of 2 into the contains method. This method returned true, which is also what we were expecting to see.

3. Mutation: Value equals to lower in range in method contains.
This mutation survived because in the original test suite conducted in assignment 3, we did not test a situation where the value is equal to upper. Therefore there was simply no mutation coverage for this method. To kill the mutation we simply created a range between (-6, 5) and then inputted a value of 5 into the contains method. This method returned true, which is also what we were expecting to see.

4. Mutation: Value equals upper in range in method contains.
This mutation survived because in the original test suite conducted in assignment 3, we did not test a situation where the value is equal to upper. Therefore there was simply no mutation coverage for this method. To kill the mutation we simply created a range between (-6, 5) and then inputted a value of 5 into the contains method. This method returned true, which is also what we were expecting to see.

5. Mutation: shift with zero crossing in range in method shift
This mutation survived because in the original test suite conducted in assignment 3, we did not test a situation where the zero crossing would be 0. Therefore to kill this method we gave the shiftWithZeroCrossing method a range of (0,0) we also gave the zero parameter to ensure that we are shifting with zero crossing.

6. Mutation: negative number shift in range in method shift
This mutation survived because in the original test suite conducted in assignment 3, we did not test a situation where the value would be negative. Due to this lack of mutation coverage we can kill this mutation by simply invoking shift with a negative value. 

7. Mutation: positive number shift in range in method shift
This mutation survived because in the original test suite conducted in assignment 3, we did not test a situation where the value would be positive. Due to this lack of mutation coverage we can kill this mutation by simply invoking shift with a positive value.  

8. Mutation: zero number shift in range in method shift
This mutation survived because in the original test suite conducted in assignment 3, we did not test a situation where the value would be zero. Due to this lack of mutation coverage we can kill this mutation by simply invoking shift with a zero value. 

9. Mutation: smaller range intersects larger range in method intersects
This mutation survived because in the original test suite conducted in assignment 3, we did not test a situation where a smaller range intersects the range of interest. Due to the lack of mutation coverage we can kill this mutation by simply creating a large range and a smaller range and then invoking the large range intersect using the value of the smaller range.

10. Mutation: l value in range in method combine
This mutation survived because in the original test suite conducted in assignment 3, we did not test a situation where the min of two ranges were being compared. Therefore this test case is designed to make sure we go through this test case properly and thus kill the mutation.

# Report all the statistics and the mutation score for each test class

**Range**

**Initial PIT Test Coverage:**

![RangeImages](./RangeImages/RangeInit.jpg)

**DataUtilities:** 

**Initial PIT Test Coverage:**


![Initial Pit Test Coverage](./Images/InitialPIT.jpg)


**2.5.6:**

**How a few equivalent mutants in the experiment can be detected automatically:**

With consideration of the tools that can be used to automate the testing of detecting equivalent mutants, machine learning algorithms can be used. To detect equivalent mutants using machine learning we have to look at the mutation logs and determine where and what mutations were made in order to help the machine learning model tell the difference between equivalent and non-equivalent mutants. Next, we need to get a bunch of mutants and decide if they are equivalent or not, moreover labeling the mutants.  We then use these labeled mutants to teach the machine learning model how to tell the difference between equivalent and non-equivalent mutants. After that, we train the machine learning model using the labeled mutants. This means we teach the model how to use the features we picked out to tell the difference between equivalent and non-equivalent mutants. Once the model is trained, we test it to see how well it can tell the difference between equivalent and non-equivalent mutants. If it works well, we can use it to automatically detect equivalent mutants in new code. This will help developers focus on the most important mutants.

**Benefits:**
The general benefit of using machine learning instead of manually finding equivalent mutants is that it generally saves time and effort. Additionally, there may be a lot of human error that can be made especially when it comes to looking over a large log that contains thousands of lines to process and examine. Since machine learning can learn patterns better than humans, it can potentially identify more equivalent mutants overall.   

**Disadvantages:**
The disadvantage of using machine learning would be the amount of labeling that has to be done to train the machine learning model to be able to determine what is equivalent and what is not. Additionally, if there are complex mutants, then the machine learning algorithm may falsely identify it. 

**Assumptions: **
An assumption of using machine learning is that the extraction and labeling process of training the machine learning algorithm is done correctly as those processes allow the model to be able to distinguish patterns and analyze data. If these processes are done incorrectly, then the model’s performance will be greatly impacted. 

**Manually Detecting Equivalent Mutants:**
Within the process of manually detecting equivalent mutants, we first begin looking at the logs and at which mutants survived and which ones were killed as finding equivalent mutants involves finding the mutants that survived that do not change the behavior of the program.

**Example of Equivalent Mutants**
Within the data utilities for loop within line 104, since the terminating condition is when i = souce.length, an equivalent mutant would be when i != source.length instead of i < source.length since the logic is still the same

![Equivalent Mutant Example](./Images/EquivalenceEX.jpg)

**Pit Test Coverage Results**

![Pit Test Coverage Results](./Images/PITResults.jpg)
 
**Discuss in your report the test cases that you had to add to increase the mutation score, and also how you designed them.**

**Data Utilities**

The way that the mutation coverage score was increased by 10% was by commenting out code that was asking for invalid inputs within the source code. An example would be within the calculateColumnTotal method within line 133 as the for loop is saying that r2=0 and the condition is when r2 > rowCount therefore the rowCount is less than zero. Since we are asking for an invalid input within the calculateColumnTotal method, the for loop is trying to iterate through the rows of the table and sum up the values in a specific column. However, the loop is set up in such a way that it will never execute if the rowCount is less than or equal to zero as an IndexOutOfBoundsError. With that in consideration, the mutants are not reachable if the portion of the code exists and must be commented out for them to be reachable. After removing it, we noticed substantial mutation coverage as it went from 73% to 83%.

![Commented Out Code 1](./Images/Comment1.jpg)

![Commented Out Code 2](./Images/Comment2.jpg)

![Commented Out Code 3](./Images/Comment3.jpg)

**Range**

![RangeFinal](./RangeImages/RangeFinal.jpg)

The way that the mutation coverage score was increased by 10% for range was by finding where there was no mutation coverage within the source code. Many times this was the reason for the mutant surviving due to the previous test cases not taking into consideration a section of a method. For example one of our methods "combine" did not have any coverage when the second input parameter was a null value. So to increase the mutation coverage we simply created a test case where the second input parameter was set to null to encompass that mutation coverage. This in turn would greatly increase the amount of mutation coverage. We continued to do this for many of the methods that resided in the range class and that enabled us to increase the amount mutation coverage from 60% to 73%.

![RangeCombine](./RangeImages/RangeCombine.jpg)

![RangeTestCombine](./RangeImages/RangeTestCombine.jpg)

# Analysis drawn on the effectiveness of each of the test classes
Based on our results, we can see that the DataUtilities class had a higher mutation coverage score. However we did not add additional test cases but more changes and fixes to the actual source code. For the Range class, more coverage was covered through the actual writing of the test cases. The RangeTest class killed more mutants than the DataUtilitiesTests.

# A discussion on the effect of equivalent mutants on mutation score accuracy
Some mutants can create issues during mutation testing due to the fact that many end up being what we call equivalent mutants. An equivalent mutant occurs when the mutant behaves the same way as the method and program it is testing. In other words the change that the mutant introduced does not have an effect on the original program being tested. This can cause the mutation score to be inaccurate to the actuality of the situation and the program being tested.

# A discussion of what could have been done to improve the mutation score of the test suites
Within the lab, we determined from our experimental process that to improve the mutation score of the test suites. First, we need to make the test cases more robust by ensuring that they cover all possible scenarios, such as edge cases and boundary conditions. Additionally, we need to look at the source code to determine areas that would never be reached by the mutants. This can be done by either removing the unreachable code or adding new test cases that cover these areas to improve the mutation score. By examining these areas highlighted by the Pit Test, we were able to determine where we needed to add coverage. This helped us to create more effective test cases and improve the overall mutation score of the test suites. Overall, by combining these approaches and continuously refining the test suite, we can achieve a higher level of confidence in the quality and robustness of our software.

# Why do we need mutation testing? Advantages and disadvantages of mutation testing
The reason why we need mutation testing is because it allows for the detection of faults in the code by testing small changes within the code. As a result of mutation testing, the quality, reliability and effectiveness of the software is greatly improved as these tests aim to evaluate areas of the code that require adjustments and improvements, while also confirming the quality of other areas.

Advantages of mutation testing helps identify areas of the test cases that may be weak within the test suite. By doing so, testers can make adjustments to ensure that the weak areas within the test suite are improved such that the overall quality of the software is improved.

Disadvantages of mutation testing is that when it comes to larger applications, it may be very resource intensive. When running the pit test on less powerful machines, it took some members up to 25 minutes to run the Pit Test. If the software was larger and had more tests, it would require more resources as large-scale software would be difficult to run for those who do not have the resources to do so. As a result, mutation testing is not accessible to everyone. 

# Explain your SELENUIM test case design process
To design our selenium test cases, we started by exploring IKEA’s main page to find different functionalities of the UI. From this, we developed tests for each functionality with a variety of inputs based on the expected user interaction and potential invalid arguments. As well, we became familiar with using Selenium to better understand the functionality and limits of the program. With this in mind, we proceeded to automate our tests by having Selenium record our actions, in which we added assertions and checkpoints to them.

# Explain the use of assertions and checkpoints
Assertions and checkpoints were both used throughout our tests, with the goal of helping us automate our tests. Assertions are statements that must evaluate to a boolean condition, such that specific results must be obtained during the test, otherwise it will halt. These were used to ensure that the correct element was returned after an automated action. On the other hand, checkpoints are used to ensure that the website is at a certain state before proceeding with the test cases. These were used primarily to ensure that the website had enough time to load the required elements before proceeding. Together, they are both used to ensure consistency, accuracy, and reliability of our automated test suites.

# How did you test each functionaity with different test data

### Different Stores:
For the different store selection the data options were extensive but were generalized into valid, invalid, expected and unexpected inputs. This way the tests would reveal if all the inputs were thought of. The first input data was valid input for a postal code in Calgary. The next was valid for the exact store. Another was an American postal code. The final one was an invalid postal code. This way all the functionalities were tested, in a general way. 

### Hamburger Menu:
The hamburger menu had many possible inputs and expected outputs. Since all the options could not be tested, a few were selected with random data from within the equivalent classes to be a general representative of the entire data options. The hamburger menu open and close features were tested. Then the buttons that test the language and sign in buttons were tested. Then some of the nested functionality of the button was also tested. This way there were an extensive number of tests that checked a lot of the functionality. 

### Search Bar: 
The search bar has many inputs and all of which cannot be tested. The input data was chosen to highlight key functionalities of the search, such as key words, id, and descriptions. The overall use idea was to test as many inputs that served a unique purpose as possible without creating too many cases.

### Shopping Cart:
The shopping cart had many inputs, but a single item was selected to be the general data input. Shopping carts can add, remove, increment, and adjust the item quantity as well as clear the cart. All these functionalities were chosen as the inputs for the shopping cart, in order to ensure that the cart worked as expected.

### Museum: 
Two tests were created for testing the IKEA Museum functionality. The first test checks to see if a video can be played that shows the backstory of IKEA. This test is called “Story of ikea”. The second test is used for checking if the language can be switched from english to swedish, asserting that an english word now reads as “IKEA Museum Älmhult”. This test is called “Switch language”.

### Review: 
We implemented two tests to test IKEA’s review functionality. The first test, called “Leave A 1 Star Review”, first searches for “RANDBÖL”, scrolls down to the review section, and proceeds to leave a review. After leaving the review, it asserts that the review was accepted by the GUI. The second test, called “Leave A 5 Star Review”, similar to the first test, searches for “VEDBÄK”, and proceeds to leave a 5 star review, asserting as well that the review was accepted by the website application.

### Sign in: 
For the sign in functionality, 4 tests were created. The first test ensures that a user can correctly sign in. It starts by asserting that the user is not currently signed in. It then proceeds to use valid credentials to sign in. Afterwards, it uses a checkpoint to ensure that the next page has time to load, where it then asserts that the user has successfully signed in. This test is called “Correct Sign In”. Similar to the first test, the second test tests the sign out feature, where it first asserts that the user is signed in, then proceeds to sign them out. It finishes by asserting that the signed out successfully. This test is called “Correct Sign Out”. The third test is similar to the first test, but provides the wrong username credential. From here, it asserts that the correct error message is displayed. This test is called “Invalid Username Sign In”. The fourth test, called “Invalid Password Sign In”, is the same as the third test, by providing an invalid password instead.

### Wishlist: 
We created two tests to test the Wishlist function of the website. Both these tests run under the assumption that the user is signed in. The first test tests adding an item to the wishlist. It starts by searching for a specific lightbulb, and asserting that it’s not already in the wishlist. Then, it adds it to the wishlist, and proceeds to go to the wishlist. Here, it asserts that the item was successfully added to the wishlist. This test is called “Add item to wishlist”. The second test removes the item from the wishlist. It runs the same as the first test by finding the specific lightbulb, where it asserts that it's in the wishlist. It proceeds to remove it from the wishlist, then heads to the wishlist to check that there are no more items in it. This test is called “Remove item from wishlist”.


# Discuss advantages and disadvantages of Selenium vs. Sikulix
### Selenium advantages:
For web application testing, it has the ability to automate actions, record and play back tests, and verify results.
Supported across Firefox and Chrome as an extension, with a simple install process.
Has more thorough testing options such as asserting specific elements and checking for certain values.
Better tool for in-depth GUI testing.
### Selenium disadvantages:
Can cause a dynamic website to slow down to an unusable state while recording tests.
### Sikulix advantages:
Uses image recognition technology.
Can be automated to have actions run across multiple applications and windows simultaneously.
### Sikulix disadvantages:
Must be downloaded from a JAR file and executed from the command line.


# How the team work/effort was divided and managed
The team was divided into pairs after selecting a portion of the assignment to complete. The groups worked independently, but shared resources, ideas and helped each other resolve issues. This way the groups could focus and meet up when they felt necessary. Everyone in the team got to experience both mutation testing and Selenium. To manage their workflow, the team used a Google doc to document their work before putting it into a GitHub repository. Additionally, they used an online chat to discuss things in real-time. Overall, the team worked collaboratively and efficiently to complete the assignment.

# Difficulties encountered, challenges overcome, and lessons learned
The main lesson learned in the project was how to use mutation testing and automated web testing. For mutation testing the first difficulty encountered was determining what the mutation metrics were referring to. After identifying in the code what the mutations were and where in the code they were impacting, the code was simple to write. Additionally some of the original code had issues, so in order to find and resolve those issues was also a challenge because it involved understanding the code on a deeper level. From that it was learned that in order to test code, in some cases you must know the purpose and functionality of it. Another challenge was the long wait times. This meant that from squashing a bug and finding out if it worked was a long process. To fix this, we determined that correcting and ensuring that the tests worked beforehand was important before running PiTest. For Selenium the main challenge was getting it to behave the same across many of the team members' computers and environments. The different browsers, operating systems, and even languages were having an impact on the test suite. So, we all downloaded the same browser and matched our settings to ensure replicability. Another issue that was found was that Selenium would unexpectedly hang or freeze. This would usually be caused by a bug that was in the script and we fixed this by minimizing the use of mouse hovers on items and using the functionality of the site and reducing the operating system inputs like shortcuts and key inputs. That way it was more replicable and less prone to freezes.

# Comments/feedback on the lab itself
Within the lab, the team found the mutation testing portion to be quite challenging and time-consuming. The large size of the test suite made it impractical to run the Pit Test in a reasonable amount of time, often taking up to 15-8 minutes to complete. This was a frustrating experience, and it would have been helpful if a smaller and more focused test suite was provided. However, despite this challenge, the team appreciated the opportunity to learn about mutation testing and the importance of test coverage. Overall, the lab was a valuable learning experience, but it would have been helpful to have more efficient tools for testing.
